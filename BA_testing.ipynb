{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM6PeX5Dn8gOnXbMoJJMaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeusGav/BA_test/blob/main/BA_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bachelor Thesis – AML Pipeline Scaffold (HI-Small / XGBoost / SHAP / LLM)\n",
        "\n",
        "This notebook sets up a clean, reproducible pipeline for my Bachelor thesis on\n",
        "an explainable, auditable Anti-Money-Laundering (AML) system based on the\n",
        "IBM / NeurIPS HI-Small dataset.\n",
        "\n",
        "The goals of this notebook are:\n",
        "\n",
        "1. Clone the GitHub repository and configure Git in Google Colab.\n",
        "2. Create a minimal, IBM/Multi-GNN-inspired project structure.\n",
        "3. Configure Kaggle access and download only the required HI-Small files.\n",
        "4. Build a standalone preprocessing script that formats the HI-Small data into a\n",
        "   tabular, ML-ready CSV file.\n",
        "5. Train a clear XGBoost baseline model and store it for later SHAP and LLM-based explanations.\n",
        "6. Commit and push all relevant scaffolding files back to GitHub.\n",
        "\n",
        "The notebook is intentionally modular and minimal to keep the repository:\n",
        "- lightweight,\n",
        "- transparent,\n",
        "- and easy to audit."
      ],
      "metadata": {
        "id": "CU2Zcsb_imNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Clone repository and configure Git"
      ],
      "metadata": {
        "id": "JOIe6m3kjIkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HbMCyC5bumv"
      },
      "outputs": [],
      "source": [
        "# Clone the GitHub repository into the Colab workspace\n",
        "!git clone https://github.com/ZeusGav/BA_test.git /content/BA_test\n",
        "\n",
        "import os\n",
        "\n",
        "# Change working directory to the cloned repository\n",
        "os.chdir(\"/content/BA_test\")\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# Configure Git user (local, per Colab session)\n",
        "!git config user.name \"ZeusGav\"\n",
        "!git config user.email \"gavras.alexios@gmail.com\"\n",
        "\n",
        "# Show current Git status for transparency\n",
        "!git status"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section clones the **BA_test** repository into the Colab environment and\n",
        "configures a Git identity. The key points are:\n",
        "\n",
        "- The repository is cloned to `/content/BA_test`, which becomes the working directory.\n",
        "- The Git user name and email are set only inside this Colab session and are used\n",
        "  for commits created from this notebook.\n",
        "- Running `git status` at the end provides immediate feedback about the current\n",
        "  state of the repository (clean, modified files, untracked files, etc.).\n",
        "\n",
        "This setup allows the notebook to:\n",
        "- create new files and folders,\n",
        "- commit them,\n",
        "- and push them back to GitHub in a reproducible way."
      ],
      "metadata": {
        "id": "F0jSt42Ki1zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Create project structure and configuration files"
      ],
      "metadata": {
        "id": "ulCvm24-jQf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "# Define the main project folders\n",
        "base_dirs = [\n",
        "    \"data\",\n",
        "    \"data/raw\",\n",
        "    \"data/processed\",\n",
        "    \"notebooks\",\n",
        "    \"preprocessing\",\n",
        "    \"src\",\n",
        "    \"models\",\n",
        "    \"shap_outputs\",\n",
        "    \"llm_outputs\",\n",
        "]\n",
        "\n",
        "# Create each folder if it does not exist yet\n",
        "for d in base_dirs:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Optionally create .gitkeep files for folders that should be visible in the repo\n",
        "tracked_empty = [\"data\", \"data/raw\", \"data/processed\", \"models\", \"shap_outputs\", \"llm_outputs\"]\n",
        "for d in tracked_empty:\n",
        "    keep_path = os.path.join(d, \".gitkeep\")\n",
        "    if not os.path.exists(keep_path):\n",
        "        open(keep_path, \"w\").close()\n",
        "\n",
        "# Create a project-specific .gitignore to keep large artifacts out of Git\n",
        "gitignore_content = textwrap.dedent(\"\"\"\n",
        "    # Python cache and build artifacts\n",
        "    __pycache__/\n",
        "    *.py[cod]\n",
        "    *.pyo\n",
        "    *.pyd\n",
        "    *.so\n",
        "    *.egg-info/\n",
        "    dist/\n",
        "    build/\n",
        "\n",
        "    # Jupyter notebook checkpoints\n",
        "    .ipynb_checkpoints/\n",
        "\n",
        "    # OS files\n",
        "    .DS_Store\n",
        "\n",
        "    # Log files\n",
        "    *.log\n",
        "\n",
        "    # Project-specific large artifacts\n",
        "    data/*\n",
        "    !data/.gitkeep\n",
        "    !data/raw/.gitkeep\n",
        "    !data/processed/.gitkeep\n",
        "\n",
        "    models/*\n",
        "    !models/.gitkeep\n",
        "\n",
        "    shap_outputs/*\n",
        "    !shap_outputs/.gitkeep\n",
        "\n",
        "    llm_outputs/*\n",
        "    !llm_outputs/.gitkeep\n",
        "\"\"\").strip() + \"\\n\"\n",
        "\n",
        "with open(\".gitignore\", \"w\") as f:\n",
        "    f.write(gitignore_content)\n",
        "\n",
        "# Central data configuration: points to the formatted AML dataset\n",
        "data_config = {\n",
        "    \"aml_data\": \"data/processed/formatted_transactions.csv\",\n",
        "    \"target_column\": \"label\"\n",
        "}\n",
        "with open(\"data_config.json\", \"w\") as f:\n",
        "    json.dump(data_config, f, indent=2)\n",
        "\n",
        "# Show resulting folder tree (one level deep)\n",
        "!ls -R"
      ],
      "metadata": {
        "id": "CglyzusleU7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section creates a **minimal and clean project scaffold**, inspired by the\n",
        "IBM/Multi-GNN repository structure but tailored for a tabular XGBoost + SHAP + LLM setup.\n",
        "\n",
        "Key design decisions:\n",
        "\n",
        "- `data/`  \n",
        "  - `data/raw/`: stores raw Kaggle HI-Small files (not tracked by Git).  \n",
        "  - `data/processed/`: stores the formatted tabular dataset for ML (not tracked).\n",
        "\n",
        "- `preprocessing/`  \n",
        "  Contains scripts that transform raw HI-Small data into a single\n",
        "  `formatted_transactions.csv` file. This plays the same conceptual role as\n",
        "  `format_kaggle_files.py` in the original Multi-GNN repo, but is implemented\n",
        "  independently and tailored to this thesis.\n",
        "\n",
        "- `src/`  \n",
        "  Contains modelling code, starting with an XGBoost baseline. Later, this will\n",
        "  be extended with SHAP-based explanations and an LLM-based explanation layer.\n",
        "\n",
        "- `models/`, `shap_outputs/`, `llm_outputs/`  \n",
        "  Store trained models and generated explanations. These directories are not\n",
        "  tracked in Git to keep the repository small and focused on code and configs.\n",
        "\n",
        "- `.gitignore`  \n",
        "  Explicitly prevents large files (datasets, models, generated outputs) from\n",
        "  being committed, which supports reproducibility and avoids polluting the\n",
        "  version history with large binary artifacts.\n",
        "\n",
        "- `data_config.json`  \n",
        "  Provides a **single source of truth** for where the AML dataset and its target\n",
        "  column are stored. This mirrors the configuration style used in the original\n",
        "  IBM/Multi-GNN codebase and makes downstream scripts more robust and readable."
      ],
      "metadata": {
        "id": "7VH-Pfu6jbZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Kaggle integration and HI-Small subset"
      ],
      "metadata": {
        "id": "2EDZCHmEjjWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pathlib\n",
        "import stat\n",
        "import zipfile\n",
        "import glob\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load Kaggle credentials from Colab user secrets\n",
        "kaggle_username = userdata.get(\"KAGGLE_USERNAME\")\n",
        "kaggle_token = userdata.get(\"KAGGLE_API_TOKEN\")\n",
        "\n",
        "if kaggle_username is None or kaggle_token is None:\n",
        "    raise ValueError(\"Please set KAGGLE_USERNAME and KAGGLE_API_TOKEN in Colab user secrets.\")\n",
        "\n",
        "# Also expose them as environment variables (used by the Kaggle CLI)\n",
        "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
        "os.environ[\"KAGGLE_API_TOKEN\"] = kaggle_token\n",
        "\n",
        "# Create the kaggle.json configuration file required by the Kaggle CLI\n",
        "kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kaggle_config = {\n",
        "    \"username\": kaggle_username,\n",
        "    \"key\": kaggle_token,\n",
        "}\n",
        "kaggle_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_path, \"w\") as f:\n",
        "    json.dump(kaggle_config, f)\n",
        "\n",
        "# Set the file permissions as required by Kaggle (600)\n",
        "kaggle_path.chmod(stat.S_IRUSR | stat.S_IWUSR)\n",
        "\n",
        "# Install Kaggle CLI if not already installed\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Define dataset and download location\n",
        "dataset = \"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\"\n",
        "raw_dir = \"data/raw\"\n",
        "os.makedirs(raw_dir, exist_ok=True)\n",
        "\n",
        "# Download only the required HI-Small files, not the full 7GB dataset\n",
        "!kaggle datasets download -d {dataset} -f HI-Small_Trans.csv -p {raw_dir} --force\n",
        "!kaggle datasets download -d {dataset} -f HI-Small_Patterns.txt -p {raw_dir} --force\n",
        "\n",
        "# Unzip any downloaded archives (Kaggle often wraps files into ZIPs)\n",
        "for z in glob.glob(os.path.join(raw_dir, \"*.zip\")):\n",
        "    with zipfile.ZipFile(z, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(raw_dir)\n",
        "    os.remove(z)\n",
        "\n",
        "# List the raw data directory to verify the download\n",
        "!ls data/raw"
      ],
      "metadata": {
        "id": "m2QmB5pCeXRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section configures Kaggle access and downloads only the **HI-Small** subset\n",
        "of the IBM transactions for Anti-Money-Laundering dataset.\n",
        "\n",
        "Important details:\n",
        "\n",
        "- Kaggle credentials are read from **Colab user secrets** (`userdata.get`), not\n",
        "  hard-coded. This keeps the notebook safer and more portable.\n",
        "- The credentials are stored in a `~/.kaggle/kaggle.json` file with restricted\n",
        "  file permissions, as required by the Kaggle CLI.\n",
        "- Only two files are downloaded:\n",
        "  - `HI-Small_Trans.csv` – the main transaction table.\n",
        "  - `HI-Small_Patterns.txt` – laundering pattern metadata (used for potential extensions).\n",
        "- We intentionally **do not download `HI-Small_Accounts.csv`**, because account-level\n",
        "  metadata is only needed for **graph-based models (e.g., Multi-GNN)**.  \n",
        "  Our pipeline uses a **pure tabular XGBoost baseline**, which relies solely on the\n",
        "  transaction table; therefore the Accounts file is not required.\n",
        "- The files are stored under `data/raw/` and are explicitly **not** tracked by Git.\n",
        "\n",
        "The motivation is to:\n",
        "- replicate the HI-Small setup used in state-of-the-art AML research,\n",
        "- avoid downloading the full 7GB dataset,\n",
        "- and decouple raw data access from the repository itself."
      ],
      "metadata": {
        "id": "L8IxeuGPjkp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Preprocessing design for HI-Small"
      ],
      "metadata": {
        "id": "oJ1v-op9jydF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_path = os.path.join(\"preprocessing\", \"format_hi_small.py\")\n",
        "\n",
        "preproc_code = \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def format_hi_small(transactions_path: str,\n",
        "                    patterns_path: str,\n",
        "                    output_path: str) -> None:\n",
        "    \\\"\\\"\\\"\n",
        "    Preprocessing for the IBM HI-Small dataset (Kaggle).\n",
        "\n",
        "    Goal:\n",
        "    - Mirror the idea of the IBM/Multi-GNN preprocessing step:\n",
        "      transform raw HI-Small files into a single, ML-ready,\n",
        "      tabular dataset (formatted_transactions.csv).\n",
        "    - Focus on a tabular XGBoost baseline and local explainability (SHAP),\n",
        "      rather than graph models.\n",
        "\n",
        "    The script performs the following steps:\n",
        "    1. Load the raw HI-Small transactions CSV.\n",
        "    2. Parse timestamps and derive simple time-based features.\n",
        "    3. Create structural flags (same account, same bank, same currency).\n",
        "    4. Define a binary target label from the 'Is Laundering' column.\n",
        "    5. Drop pure identifier columns from the feature space.\n",
        "    6. Apply one-hot encoding to key categorical attributes.\n",
        "    7. Save the result as a CSV file that is directly usable by ML models.\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    # 1) Load transaction data\n",
        "    df = pd.read_csv(transactions_path)\n",
        "\n",
        "    if \"Is Laundering\" not in df.columns:\n",
        "        raise ValueError(\"Column 'Is Laundering' not found in transactions file.\")\n",
        "\n",
        "    # 1b) Add a stable transaction identifier for downstream SHAP/LLM/UI\n",
        "    # Use the original row index so each transaction gets a unique ID.\n",
        "    df[\"transaction_id\"] = df.index\n",
        "\n",
        "    # 2) Convert timestamp to datetime and derive simple time features\n",
        "    if \"Timestamp\" in df.columns:\n",
        "        df[\"Timestamp\"] = pd.to_datetime(\n",
        "            df[\"Timestamp\"],\n",
        "            format=\"%Y/%m/%d %H:%M\",\n",
        "            errors=\"coerce\"\n",
        "        )\n",
        "        df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
        "        df[\"Weekday\"] = df[\"Timestamp\"].dt.dayofweek\n",
        "        df[\"Weekend\"] = (df[\"Weekday\"] >= 5).astype(int)\n",
        "    else:\n",
        "        raise ValueError(\"Column 'Timestamp' not found – please verify the dataset format.\")\n",
        "\n",
        "    # 3) Structural relationship flags (accounts, banks, currencies)\n",
        "    if {\"Account\", \"Account.1\"}.issubset(df.columns):\n",
        "        df[\"SameAccount\"] = (df[\"Account\"] == df[\"Account.1\"]).astype(int)\n",
        "    else:\n",
        "        df[\"SameAccount\"] = 0\n",
        "\n",
        "    if {\"From Bank\", \"To Bank\"}.issubset(df.columns):\n",
        "        df[\"SameBank\"] = (df[\"From Bank\"] == df[\"To Bank\"]).astype(int)\n",
        "    else:\n",
        "        df[\"SameBank\"] = 0\n",
        "\n",
        "    if {\"Receiving Currency\", \"Payment Currency\"}.issubset(df.columns):\n",
        "        df[\"SameCurrency\"] = (df[\"Receiving Currency\"] == df[\"Payment Currency\"]).astype(int)\n",
        "    else:\n",
        "        df[\"SameCurrency\"] = 0\n",
        "\n",
        "    # 4) Define the target label from 'Is Laundering'\n",
        "    df[\"label\"] = df[\"Is Laundering\"].astype(int)\n",
        "\n",
        "    # 5) Remove pure identifier columns from the feature space\n",
        "    id_cols = [\n",
        "        \"Account\",\n",
        "        \"Account.1\",\n",
        "        \"From Bank\",\n",
        "        \"To Bank\",\n",
        "    ]\n",
        "\n",
        "    drop_cols = [c for c in id_cols if c in df.columns]\n",
        "    drop_cols.append(\"Timestamp\")  # we already extracted time-based features\n",
        "\n",
        "    drop_cols = [c for c in drop_cols if c in df.columns]\n",
        "\n",
        "    target_col = \"label\"\n",
        "    feature_df = df.drop(columns=drop_cols + [target_col, \"Is Laundering\"])\n",
        "\n",
        "    # 6) One-hot encode key categorical columns\n",
        "    categorical_cols = []\n",
        "    for col in [\"Receiving Currency\", \"Payment Currency\", \"Payment Format\"]:\n",
        "        if col in feature_df.columns:\n",
        "            categorical_cols.append(col)\n",
        "\n",
        "    feature_df = pd.get_dummies(feature_df, columns=categorical_cols, dummy_na=False)\n",
        "\n",
        "    # Reattach the label as the first column\n",
        "    out_df = pd.concat(\n",
        "        [df[target_col].reset_index(drop=True), feature_df.reset_index(drop=True)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # 7) Save formatted dataset\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    out_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Formatted dataset saved to: {output_path}\")\n",
        "    print(f\"Shape: {out_df.shape}\")\n",
        "    print(f\"Positive class share: {out_df['label'].mean():.6f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Format IBM HI-Small Kaggle files into an ML-ready CSV.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--transactions_path\",\n",
        "        type=str,\n",
        "        default=\"data/raw/HI-Small_Trans.csv\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--patterns_path\",\n",
        "        type=str,\n",
        "        default=\"data/raw/HI-Small_Patterns.txt\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_path\",\n",
        "        type=str,\n",
        "        default=\"data/processed/formatted_transactions.csv\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    format_hi_small(\n",
        "        transactions_path=args.transactions_path,\n",
        "        patterns_path=args.patterns_path,\n",
        "        output_path=args.output_path,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open(preproc_path, \"w\") as f:\n",
        "    f.write(preproc_code)\n",
        "\n",
        "!sed -n '1,80p' preprocessing/format_hi_small.py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CPiILvCjej4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing script `preprocessing/format_hi_small.py` is the core of the\n",
        "data preparation pipeline. It is:\n",
        "\n",
        "- **Inspired by** the original IBM/Multi-GNN preprocessing,\n",
        "- but **implemented from scratch** to fit a tabular XGBoost + SHAP + LLM workflow.\n",
        "\n",
        "Key design choices:\n",
        "\n",
        "1. **Timestamp handling**  \n",
        "   The raw `Timestamp` column is parsed into a proper datetime object and then\n",
        "   decomposed into simple, interpretable features: `Hour`, `Weekday`, `Weekend`.\n",
        "   The original timestamp is removed from the feature space to avoid leaking raw\n",
        "   time indices and to keep the representation compact.\n",
        "\n",
        "2. **Structural flags**  \n",
        "   Binary flags (such as `SameAccount`, `SameBank`, `SameCurrency`) encode basic\n",
        "   structural relationships within each transaction. These features are cheap to\n",
        "   compute, easy to interpret, and often highly relevant in AML settings.\n",
        "\n",
        "3. **Target definition**  \n",
        "   The binary label `label` is directly derived from the `Is Laundering` column\n",
        "   provided in HI-Small. This makes the target construction transparent and easy\n",
        "   to justify.\n",
        "\n",
        "4. **Identifier removal**  \n",
        "   Pure identifiers (account IDs, bank IDs) are removed from the feature space.\n",
        "   This avoids data leakage and discourages the model from learning spurious,\n",
        "   non-generalizable patterns based on IDs.\n",
        "\n",
        "5. **Categorical encoding**  \n",
        "   Key categorical columns (currencies and payment format) are one-hot encoded.\n",
        "   This is a standard, model-agnostic representation and works well with\n",
        "   gradient-boosted trees.\n",
        "\n",
        "The resulting `formatted_transactions.csv` is a **single, self-contained,\n",
        "tabular dataset** that can be used by the XGBoost baseline and later for SHAP\n",
        "explanations and LLM-generated narratives."
      ],
      "metadata": {
        "id": "ZUXGcC7Ejv2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Run preprocessing to create formatted_transactions.csv\n"
      ],
      "metadata": {
        "id": "Yd-tv1gskfjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocessing/format_hi_small.py \\\n",
        "    --transactions_path data/raw/HI-Small_Trans.csv \\\n",
        "    --patterns_path data/raw/HI-Small_Patterns.txt \\\n",
        "    --output_path data/processed/formatted_transactions.csv\n",
        "\n",
        "# Inspect the processed data directory\n",
        "!ls -l data/processed"
      ],
      "metadata": {
        "id": "QkY3IKEMel-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Build XGBoost baseline training script\n"
      ],
      "metadata": {
        "id": "8RfSOtEzkFM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_train_path = os.path.join(\"src\", \"train_xgboost_baseline.py\")\n",
        "\n",
        "train_code = \"\"\"\n",
        "import json\n",
        "import os\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Load configuration for AML dataset path and target column\n",
        "    with open(\"data_config.json\", \"r\") as f:\n",
        "        cfg = json.load(f)\n",
        "\n",
        "    data_path = cfg.get(\"aml_data\", \"data/processed/formatted_transactions.csv\")\n",
        "    target_column = cfg.get(\"target_column\", \"label\")\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        raise FileNotFoundError(f\"Formatted dataset not found at: {data_path}\")\n",
        "\n",
        "    # 2) Load data and split into features and target\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    if target_column not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
        "\n",
        "    drop_cols = [target_column]\n",
        "    if \"transaction_id\" in df.columns:\n",
        "        drop_cols.append(\"transaction_id\")\n",
        "\n",
        "    X = df.drop(columns=drop_cols)\n",
        "    y = df[target_column].astype(int)\n",
        "\n",
        "    # 3) Train/test split (stratified to respect class imbalance)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y,\n",
        "    )\n",
        "\n",
        "    # 4) Handle class imbalance via scale_pos_weight\n",
        "    pos = y_train.sum()\n",
        "    neg = len(y_train) - pos\n",
        "    scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
        "\n",
        "    # 5) Define a clear, interpretable XGBoost baseline model\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=400,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric=\"logloss\",\n",
        "        tree_method=\"hist\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 6) Evaluate using ROC-AUC and PR-AUC (average precision)\n",
        "    proba_test = model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, proba_test)\n",
        "    pr_auc = average_precision_score(y_test, proba_test)\n",
        "\n",
        "    print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"Test PR-AUC : {pr_auc:.4f}\")\n",
        "    print(f\"Positive share train: {y_train.mean():.6f}, test: {y_test.mean():.6f}\")\n",
        "\n",
        "    # 7) Save model and metrics for later SHAP and LLM-based explanations\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    model_path = os.path.join(\"models\", \"xgb_hi_small_baseline.pkl\")\n",
        "    metrics_path = os.path.join(\"models\", \"xgb_hi_small_baseline_metrics.json\")\n",
        "\n",
        "    joblib.dump(model, model_path)\n",
        "\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump({\"roc_auc\": roc_auc, \"pr_auc\": pr_auc}, f, indent=2)\n",
        "\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "    print(f\"Metrics saved to: {metrics_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open(src_train_path, \"w\") as f:\n",
        "    f.write(train_code)\n",
        "\n",
        "# Show the top of the training script for verification\n",
        "!sed -n '1,80p' src/train_xgboost_baseline.py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yUInng13eoAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script `src/train_xgboost_baseline.py` defines a strong, yet transparent\n",
        "baseline model:\n",
        "\n",
        "- It reads the AML dataset path and target column from `data_config.json`.\n",
        "- It uses a **stratified train/test split**, which is important for highly\n",
        "  imbalanced AML data.\n",
        "- It computes `scale_pos_weight` to account for class imbalance in XGBoost.\n",
        "\n",
        "The evaluation metrics are:\n",
        "\n",
        "- **ROC-AUC**  \n",
        "  Measures ranking quality across all thresholds. It is robust to class imbalance.\n",
        "\n",
        "- **PR-AUC (Average Precision)**  \n",
        "  Measures precision-recall tradeoff and is more informative in highly imbalanced\n",
        "  settings. The raw PR-AUC value must be interpreted relative to the positive\n",
        "  class share (baseline precision).\n",
        "\n",
        "The trained model and a small JSON file with metrics are saved under `models/`,\n",
        "which allows:\n",
        "\n",
        "- downstream SHAP analysis, and\n",
        "- an LLM-based explanation layer that can reference the exact model version and metrics.\n",
        "\n",
        "This baseline is not meant to be over-tuned. Instead, it provides a **solid and\n",
        "reproducible starting point** to study explainability and auditability."
      ],
      "metadata": {
        "id": "fNUo8YCgkX0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Install dependencies, train XGBoost baseline, and inspect outputs\n"
      ],
      "metadata": {
        "id": "4LrxlH4-ka6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required Python packages\n",
        "!pip install -q xgboost joblib scikit-learn\n",
        "\n",
        "# Train the XGBoost baseline model\n",
        "!python src/train_xgboost_baseline.py\n",
        "\n",
        "# Inspect the models directory\n",
        "!ls -l models"
      ],
      "metadata": {
        "id": "i4Z2PtzyeqyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Sanity checks"
      ],
      "metadata": {
        "id": "sL3hSAMkSCJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "def sanity_check(path=\"data/processed/formatted_transactions.csv\"):\n",
        "    print(\"\\n=== SANITY CHECK: formatted_transactions.csv ===\")\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[FAIL] File not found: {path}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    print(\"[INFO] Loaded:\", path)\n",
        "    print(\"[INFO] Shape:\", df.shape)\n",
        "\n",
        "    # 1) Check presence of key columns\n",
        "    required_cols = [\n",
        "        \"label\",\n",
        "        \"transaction_id\",\n",
        "        \"Hour\", \"Weekday\", \"Weekend\",\n",
        "        \"SameAccount\", \"SameBank\", \"SameCurrency\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n[CHECK] Required columns present?\")\n",
        "    for col in required_cols:\n",
        "        print(f\" - {col}: {'OK' if col in df.columns else 'MISSING'}\")\n",
        "\n",
        "    # 2) Check transaction_id is unique\n",
        "    print(\"\\n[CHECK] transaction_id uniqueness:\")\n",
        "    if df[\"transaction_id\"].is_unique:\n",
        "        print(\" - OK: transaction_id is unique\")\n",
        "    else:\n",
        "        print(\" - FAIL: transaction_id is NOT unique!\")\n",
        "\n",
        "    # 3) Check class balance\n",
        "    pos_rate = df[\"label\"].mean()\n",
        "    print(f\"\\n[CHECK] Positive class share: {pos_rate:.6f}\")\n",
        "    if 0 < pos_rate < 0.01:\n",
        "        print(\" - OK: class imbalance looks reasonable\")\n",
        "    else:\n",
        "        print(\" - WARNING: suspicious positive rate\")\n",
        "\n",
        "    # 4) Check one-hot encoding worked\n",
        "    oh_cols_currency = [c for c in df.columns if \"Currency_\" in c]\n",
        "    oh_cols_format = [c for c in df.columns if \"Payment Format_\" in c]\n",
        "\n",
        "    print(\"\\n[CHECK] One-hot encoded columns:\")\n",
        "    print(f\" - Currency columns: {len(oh_cols_currency)} found\")\n",
        "    print(f\" - Payment Format columns: {len(oh_cols_format)} found\")\n",
        "    if len(oh_cols_currency) == 0 or len(oh_cols_format) == 0:\n",
        "        print(\" - FAIL: One-hot encoding seems missing\")\n",
        "    else:\n",
        "        print(\" - OK: one-hot encoding present\")\n",
        "\n",
        "    # 5) Check no raw ID columns leaked into final dataset\n",
        "    forbidden = [\"Account\", \"Account.1\", \"From Bank\", \"To Bank\"]\n",
        "    print(\"\\n[CHECK] Forbidden ID columns:\")\n",
        "    leaked = [c for c in forbidden if c in df.columns]\n",
        "    if len(leaked) == 0:\n",
        "        print(\" - OK: no raw identifier columns present\")\n",
        "    else:\n",
        "        print(\" - FAIL: raw ID columns found:\", leaked)\n",
        "\n",
        "    print(\"\\n=== SANITY CHECK COMPLETE ===\\n\")\n",
        "\n",
        "\n",
        "# Run the sanity check\n",
        "sanity_check()"
      ],
      "metadata": {
        "id": "Mj2cM3kOSCha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section performs a set of lightweight validation checks on the processed\n",
        "formatted_transactions.csv file to ensure that the preprocessing pipeline\n",
        "executed correctly and produced a clean, ML-ready dataset.\n",
        "\n",
        "The sanity check verifies that:\n",
        "- all required engineered features (Hour, Weekday, Weekend, SameAccount, SameBank, SameCurrency) are present,\n",
        "- the newly added transaction_id column is unique and stable,\n",
        "- the positive class share matches the expected laundering rate of HI-Small,\n",
        "- one-hot encoding for currencies and payment formats was applied correctly,\n",
        "- and no raw identifier columns (e.g., Account, Account.1, From Bank, To Bank) remain in the final dataset.\n",
        "\n",
        "These checks help confirm that the dataset is consistent, free of leakage,\n",
        "and ready for downstream steps such as model training, SHAP explainability, and LLM-based auditing.\n"
      ],
      "metadata": {
        "id": "x4Amwu4USWIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: SHAP Setup – Preparing Data and Model for Explanations"
      ],
      "metadata": {
        "id": "xv3Nye1od5_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SHAP if needed (Colab-friendly; no-op if already installed)\n",
        "!pip install shap --quiet\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "\n",
        "# --- SHAP output directories (kept out of Git via .gitignore) ---\n",
        "SHAP_GLOBAL_DIR = \"shap_outputs/global\"\n",
        "SHAP_LOCAL_DIR = \"shap_outputs/local\"\n",
        "\n",
        "os.makedirs(SHAP_GLOBAL_DIR, exist_ok=True)\n",
        "os.makedirs(SHAP_LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"[INFO] SHAP output directories:\")\n",
        "print(\" -\", SHAP_GLOBAL_DIR)\n",
        "print(\" -\", SHAP_LOCAL_DIR)\n",
        "\n",
        "# --- Load central data configuration ---\n",
        "with open(\"data_config.json\", \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "data_path = cfg[\"aml_data\"]\n",
        "target_col = cfg[\"target_column\"]\n",
        "\n",
        "print(\"\\n[INFO] Data configuration:\")\n",
        "print(\" - aml_data      :\", data_path)\n",
        "print(\" - target_column :\", target_col)\n",
        "\n",
        "# --- Load trained XGBoost model ---\n",
        "model_path = os.path.join(\"models\", \"xgb_hi_small_baseline.pkl\")\n",
        "model = joblib.load(model_path)\n",
        "print(\"\\n[INFO] Loaded XGBoost model from:\", model_path)\n",
        "\n",
        "# --- Load full formatted AML dataset ---\n",
        "df_full = pd.read_csv(data_path)\n",
        "print(\"\\n[INFO] Loaded formatted dataset:\")\n",
        "print(\" - shape:\", df_full.shape)\n",
        "\n",
        "# --- Separate features, target and transaction_id ---\n",
        "if \"transaction_id\" not in df_full.columns:\n",
        "    raise ValueError(\"Expected 'transaction_id' column in formatted dataset, but it is missing.\")\n",
        "\n",
        "feature_cols = [\n",
        "    c for c in df_full.columns\n",
        "    if c not in [target_col, \"transaction_id\"]\n",
        "]\n",
        "\n",
        "X_full = df_full[feature_cols]\n",
        "y_full = df_full[target_col]\n",
        "transaction_ids = df_full[\"transaction_id\"].values\n",
        "\n",
        "print(\"\\n[INFO] Feature matrix and target:\")\n",
        "print(\" - X_full shape :\", X_full.shape)\n",
        "print(\" - y_full shape :\", y_full.shape)\n",
        "print(\" - #features    :\", len(feature_cols))\n",
        "\n",
        "# Initialize SHAP JS visualization support (for notebook plots later)\n",
        "shap.initjs()"
      ],
      "metadata": {
        "id": "i_2HPYOqeEcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SHAP setup cell prepares all components required for model explainability.\n",
        "\n",
        "- It installs and imports the SHAP library, ensuring compatibility with the Colab\n",
        "  environment.\n",
        "- Two output directories (`shap_outputs/global` and `shap_outputs/local`) are\n",
        "  created. These folders store SHAP values, metadata, and plots, and are excluded\n",
        "  from version control to avoid large binary files.\n",
        "\n",
        "The script then loads the central configuration file `data_config.json`, which\n",
        "specifies:\n",
        "- the path to the formatted AML dataset, and\n",
        "- the name of the binary target column.\n",
        "\n",
        "Next, it loads the previously trained XGBoost model from `models/`. This ensures\n",
        "that global and local SHAP analysis is always tied to a specific, reproducible\n",
        "model version.\n",
        "\n",
        "The formatted transactions dataset (≈5 million rows) is then loaded, and the code\n",
        "constructs:\n",
        "- a clean feature matrix (45 engineered variables),\n",
        "- the target vector (`label`), and\n",
        "- the `transaction_id` column, which is essential for per-transaction explanations.\n",
        "\n",
        "Finally, SHAP’s visualization support is initialized, enabling interactive\n",
        "summary and force plots later in the notebook.\n",
        "\n",
        "This setup establishes a **reliable and auditable foundation** for computing both\n",
        "global and local SHAP explanations in the following sections."
      ],
      "metadata": {
        "id": "6CgbOBuceUVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 10: Global SHAP – Sampling and Computing Values"
      ],
      "metadata": {
        "id": "1vFE6kc2xK7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global SHAP configuration ---\n",
        "GLOBAL_SAMPLE_SIZE = 5000\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "n_rows = X_full.shape[0]\n",
        "sample_size = min(GLOBAL_SAMPLE_SIZE, n_rows)\n",
        "\n",
        "rng = np.random.RandomState(RANDOM_STATE)\n",
        "sample_indices = rng.choice(n_rows, size=sample_size, replace=False)\n",
        "\n",
        "X_sample = X_full.iloc[sample_indices]\n",
        "y_sample = y_full.iloc[sample_indices]\n",
        "transaction_ids_sample = transaction_ids[sample_indices]\n",
        "\n",
        "print(\"[INFO] Global SHAP sample:\")\n",
        "print(\" - sample_size           :\", sample_size)\n",
        "print(\" - laundering share (y=1):\", y_sample.mean())\n",
        "\n",
        "# --- Initialize TreeExplainer for XGBoost model ---\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# For binary classification with XGBoost, this returns\n",
        "# an array of shape (n_samples, n_features)\n",
        "print(\"\\n[INFO] Computing SHAP values for global sample ...\")\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "expected_value = explainer.expected_value\n",
        "\n",
        "print(\"[INFO] Done. SHAP values shape:\", np.array(shap_values).shape)\n",
        "\n",
        "# --- Persist SHAP artifacts for reproducibility (not tracked by git) ---\n",
        "global_prefix = os.path.join(SHAP_GLOBAL_DIR, \"hi_small_global\")\n",
        "\n",
        "np.save(global_prefix + \"_shap_values.npy\", shap_values)\n",
        "np.save(global_prefix + \"_X_sample.npy\", X_sample.values)\n",
        "np.save(global_prefix + \"_transaction_ids.npy\", transaction_ids_sample)\n",
        "\n",
        "with open(global_prefix + \"_feature_names.json\", \"w\") as f:\n",
        "    json.dump(feature_cols, f)\n",
        "\n",
        "meta = {\n",
        "    \"model_path\": model_path,\n",
        "    \"data_path\": data_path,\n",
        "    \"sample_size\": int(sample_size),\n",
        "    \"random_state\": int(RANDOM_STATE),\n",
        "    \"expected_value\": float(expected_value) if np.isscalar(expected_value)\n",
        "                     else [float(ev) for ev in np.ravel(expected_value)],\n",
        "    \"n_features\": len(feature_cols),\n",
        "}\n",
        "with open(global_prefix + \"_meta.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(\"\\n[INFO] Saved global SHAP artifacts under:\", SHAP_GLOBAL_DIR)"
      ],
      "metadata": {
        "id": "GXDuXTK-fglw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell computes global SHAP values on a fixed, reproducible sample of\n",
        "transactions.\n",
        "\n",
        "- It draws a random sample of up to 5,000 rows using a fixed random seed to\n",
        "  ensure reproducibility.\n",
        "- It initializes a `TreeExplainer` for the trained XGBoost model and computes\n",
        "  SHAP values for the sampled feature matrix.\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "- It then saves all relevant artifacts under `shap_outputs/global/`:\n",
        "  - SHAP values for the sample,\n",
        "  - the sampled feature matrix,\n",
        "  - the corresponding `transaction_id`s,\n",
        "  - the list of feature names, and\n",
        "  - a small JSON metadata file (model path, data path, sample size, expected\n",
        "    value, random seed, number of features).\n",
        "\n",
        "These files are not tracked by Git and provide a reproducible basis for global\n",
        "feature-importance analysis and later auditability."
      ],
      "metadata": {
        "id": "_K5VhgLkfxt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 11: Global SHAP – Visualization and Summary Statistics\n"
      ],
      "metadata": {
        "id": "46jwgnTO13P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Select SHAP values for plotting (handle list output for binary classification) ---\n",
        "if isinstance(shap_values, list):\n",
        "    shap_plot_values = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
        "else:\n",
        "    shap_plot_values = shap_values\n",
        "\n",
        "MAX_DISPLAY = 20  # limit number of displayed features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1) BAR SUMMARY PLOT (white background, resized)\n",
        "# ----------------------------------------------------------------------\n",
        "shap.summary_plot(\n",
        "    shap_plot_values,\n",
        "    X_sample,\n",
        "    feature_names=feature_cols,\n",
        "    plot_type=\"bar\",\n",
        "    max_display=MAX_DISPLAY,\n",
        "    show=False,\n",
        ")\n",
        "\n",
        "fig = plt.gcf()\n",
        "ax = plt.gca()\n",
        "\n",
        "# resize\n",
        "fig.set_size_inches(9, 4)\n",
        "\n",
        "# white theme (default)\n",
        "fig.patch.set_facecolor(\"white\")\n",
        "ax.set_facecolor(\"white\")\n",
        "\n",
        "# smaller labels, cleaner style\n",
        "ax.tick_params(labelsize=8)\n",
        "for label in ax.get_yticklabels():\n",
        "    label.set_fontsize(8)\n",
        "for label in ax.get_xticklabels():\n",
        "    label.set_fontsize(8)\n",
        "\n",
        "ax.set_title(\"Global Feature Importance (Mean |SHAP|)\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "bar_png = os.path.join(SHAP_GLOBAL_DIR, \"hi_small_global_summary_bar.png\")\n",
        "bar_svg = os.path.join(SHAP_GLOBAL_DIR, \"hi_small_global_summary_bar.svg\")\n",
        "plt.savefig(bar_png, dpi=200, bbox_inches=\"tight\")\n",
        "plt.savefig(bar_svg, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2) DOT SUMMARY PLOT (white background, resized)\n",
        "# ----------------------------------------------------------------------\n",
        "shap.summary_plot(\n",
        "    shap_plot_values,\n",
        "    X_sample,\n",
        "    feature_names=feature_cols,\n",
        "    max_display=MAX_DISPLAY,\n",
        "    show=False,\n",
        ")\n",
        "\n",
        "fig = plt.gcf()\n",
        "ax = plt.gca()\n",
        "\n",
        "fig.set_size_inches(9, 4)\n",
        "\n",
        "# white theme (default)\n",
        "fig.patch.set_facecolor(\"white\")\n",
        "ax.set_facecolor(\"white\")\n",
        "\n",
        "ax.tick_params(labelsize=8)\n",
        "for label in ax.get_yticklabels():\n",
        "    label.set_fontsize(8)\n",
        "for label in ax.get_xticklabels():\n",
        "    label.set_fontsize(8)\n",
        "\n",
        "ax.set_title(\"Global SHAP Impact (Top Features)\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "dot_png = os.path.join(SHAP_GLOBAL_DIR, \"hi_small_global_summary_dot.png\")\n",
        "dot_svg = os.path.join(SHAP_GLOBAL_DIR, \"hi_small_global_summary_dot.svg\")\n",
        "plt.savefig(dot_png, dpi=200, bbox_inches=\"tight\")\n",
        "plt.savefig(dot_svg, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "print(\"[INFO] Saved clean white-background global SHAP plots:\")\n",
        "print(\" -\", bar_png)\n",
        "print(\" -\", bar_svg)\n",
        "print(\" -\", dot_png)\n",
        "print(\" -\", dot_svg)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sBt0lJ9P15Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section visualizes and summarizes the global SHAP values computed in the\n",
        "previous step. Two plots are generated:\n",
        "\n",
        "- a **bar summary plot** showing the overall importance (mean |SHAP|) of each\n",
        "  feature, and\n",
        "- a **dot summary plot** showing how individual feature values push model\n",
        "  predictions toward *normal* or *suspicious*.\n",
        "\n",
        "Both plots are resized for readability and limited to the **top 20 features** to\n",
        "avoid label overlap. Each figure is exported as both PNG and SVG, allowing for\n",
        "high-quality embedding in the thesis.\n",
        "\n",
        "In addition, a compact **feature-importance table** is computed from the mean\n",
        "absolute SHAP value per feature. This provides a numeric ranking that can be\n",
        "directly referenced in the results chapter.\n",
        "\n",
        "These global visualizations are independent of the local SHAP → LLM workflow,\n",
        "but they offer a clear overview of the model’s main drivers and support the\n",
        "interpretation of the overall AML detection behaviour."
      ],
      "metadata": {
        "id": "qeckeh-_1-OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 12: Local SHAP – Per-Transaction Explanations"
      ],
      "metadata": {
        "id": "2_q8hS24AR6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def explain_transactions_local(transaction_ids, top_n=5, save_json=True):\n",
        "\n",
        "    # Normalize input to a list\n",
        "    if isinstance(transaction_ids, (int, float, str)):\n",
        "        tx_ids_list = [transaction_ids]\n",
        "    else:\n",
        "        tx_ids_list = list(transaction_ids)\n",
        "\n",
        "    # Ensure transaction_ids are of same type as in df_full[\"transaction_id\"]\n",
        "    # (in HI-Small, transaction_id is an integer index, but we keep this generic)\n",
        "    tx_ids_set = set(tx_ids_list)\n",
        "\n",
        "    # Filter dataset for requested transaction_ids\n",
        "    mask = df_full[\"transaction_id\"].isin(tx_ids_set)\n",
        "    df_local = df_full.loc[mask].copy()\n",
        "\n",
        "    if df_local.empty:\n",
        "        raise ValueError(f\"No rows found for transaction_ids: {tx_ids_list}\")\n",
        "\n",
        "    # Preserve original order of requested transaction_ids where possible\n",
        "    df_local = df_local.set_index(\"transaction_id\").loc[tx_ids_list]\n",
        "    X_local = df_local[feature_cols]\n",
        "    transaction_ids_local = df_local.index.tolist()\n",
        "\n",
        "    # Model predictions (probability of positive class)\n",
        "    proba = model.predict_proba(X_local)[:, 1]\n",
        "\n",
        "    # Local SHAP values for these rows\n",
        "    shap_local = explainer.shap_values(X_local)\n",
        "    if isinstance(shap_local, list):\n",
        "        # For binary classification, use SHAP values for the positive class\n",
        "        shap_local = shap_local[1] if len(shap_local) > 1 else shap_local[0]\n",
        "\n",
        "    shap_local = np.array(shap_local)  # shape: (n_rows, n_features)\n",
        "    n_rows, n_features = shap_local.shape\n",
        "\n",
        "    if top_n > n_features:\n",
        "        top_n = n_features\n",
        "\n",
        "    explanations = {}\n",
        "\n",
        "    for i, tx_id in enumerate(transaction_ids_local):\n",
        "        shap_row = shap_local[i]\n",
        "        feature_values_row = X_local.iloc[i]\n",
        "\n",
        "        # Sort features by absolute SHAP value (descending)\n",
        "        sorted_idx = np.argsort(-np.abs(shap_row))\n",
        "        top_idx = sorted_idx[:top_n]\n",
        "\n",
        "        top_features = []\n",
        "        for j in top_idx:\n",
        "            fname = feature_cols[j]\n",
        "            fval = feature_values_row[fname]\n",
        "            fshap = shap_row[j]\n",
        "            top_features.append({\n",
        "                \"feature\": fname,\n",
        "                \"value\": float(fval) if np.isscalar(fval) else fval,\n",
        "                \"shap_value\": float(fshap),\n",
        "            })\n",
        "\n",
        "        all_features_sorted = [feature_cols[j] for j in sorted_idx]\n",
        "\n",
        "        explanations[tx_id] = {\n",
        "            \"probability\": float(proba[i]),\n",
        "            \"top_features\": top_features,\n",
        "            \"all_features_sorted\": all_features_sorted,\n",
        "        }\n",
        "\n",
        "        # Optionally persist explanation as JSON (one file per transaction)\n",
        "        if save_json:\n",
        "            local_path = os.path.join(\n",
        "                SHAP_LOCAL_DIR,\n",
        "                f\"hi_small_local_tx_{tx_id}.json\"\n",
        "            )\n",
        "            with open(local_path, \"w\") as f:\n",
        "                json.dump(explanations[tx_id], f, indent=2)\n",
        "\n",
        "    print(\"[INFO] Computed local SHAP explanations for transaction_ids:\")\n",
        "    print(\" \", transaction_ids_local)\n",
        "    if save_json:\n",
        "        print(\"[INFO] Saved individual JSON explanations under:\", SHAP_LOCAL_DIR)\n",
        "\n",
        "    return explanations"
      ],
      "metadata": {
        "id": "nRmLEmglAUoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (Optional) Sanity Check: Run Local SHAP on a Few Transactions\n",
        "# Pick a few transaction_ids from the SHAP sample or from df_full\n",
        "test_ids = transaction_ids_sample[:3].tolist()  # e.g. first 3 from global sample\n",
        "\n",
        "local_expl = explain_transactions_local(test_ids, top_n=5)\n",
        "local_expl"
      ],
      "metadata": {
        "id": "MemS5p08A2i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section implements a reusable “local SHAP engine” that computes model\n",
        "explanations for one or multiple transactions identified by their\n",
        "`transaction_id`.\n",
        "\n",
        "The main entry point is:\n",
        "\n",
        "### `explain_transactions_local(transaction_ids, top_n=5, save_json=True)`\n",
        "\n",
        "**Parameters**\n",
        "- **transaction_ids**:  \n",
        "  Either a single transaction ID (int/str) or a list of IDs.  \n",
        "  Each ID is looked up inside the formatted AML dataset.\n",
        "\n",
        "- **top_n** (default = 5):  \n",
        "  Number of features with the highest absolute SHAP impact to include in the\n",
        "  summary. This creates a compact explanation suitable for human consumption\n",
        "  and LLM prompts.\n",
        "\n",
        "- **save_json** (default = True):  \n",
        "  If enabled, each transaction’s explanation is saved as an audit-ready JSON\n",
        "  file under `shap_outputs/local/`.\n",
        "\n",
        "**Returned object**\n",
        "\n",
        "The function returns a dictionary keyed by `transaction_id`.  \n",
        "For each transaction, the structure contains:\n",
        "\n",
        "- **\"probability\"** – predicted laundering probability from the XGBoost model  \n",
        "- **\"top_features\"** – list of the most influential features (name, value,\n",
        "  SHAP impact)  \n",
        "- **\"all_features_sorted\"** – full list of feature names ranked by absolute\n",
        "  SHAP value (useful for debugging or extended analysis)\n",
        "\n",
        "This local SHAP engine provides the standardized explanation object that will\n",
        "later feed directly into the LLM explanation layer and, optionally, into a UI."
      ],
      "metadata": {
        "id": "ROuiq2osAYa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 13: LLM Explanation Engine – Turning SHAP into Text\n"
      ],
      "metadata": {
        "id": "bHemNNpGCC43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Section 13: LLM Explanation Engine (GPT-5 Responses API)\n",
        "# ================================\n",
        "\n",
        "!pip install --quiet openai\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# Directory for LLM outputs (not tracked by git)\n",
        "LLM_LOCAL_DIR = \"llm_outputs/local\"\n",
        "os.makedirs(LLM_LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "# Load API key from Colab user secrets\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if openai_key is None:\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY in Colab user secrets.\")\n",
        "\n",
        "client = OpenAI(api_key=openai_key)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Prompt Builder (strict format for consistent, clean explanations)\n",
        "# --------------------------------------------------------------------\n",
        "def _build_llm_prompt_for_transaction(tx_id, expl_entry):\n",
        "    \"\"\"Create the AML explanation prompt from SHAP values.\"\"\"\n",
        "    prob = expl_entry[\"probability\"]\n",
        "    top_feats = expl_entry[\"top_features\"]\n",
        "\n",
        "    feature_lines = []\n",
        "    for f in top_feats:\n",
        "        direction = \"increases\" if f[\"shap_value\"] > 0 else \"decreases\"\n",
        "        feature_lines.append(\n",
        "            f\"- {f['feature']} = {f['value']} \"\n",
        "            f\"(impact: {direction} suspicion)\"\n",
        "        )\n",
        "\n",
        "    features_block = \"\\n\".join(feature_lines)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an AML analyst explaining the output of a binary classifier used for\n",
        "money-laundering detection.\n",
        "\n",
        "Explain briefly:\n",
        "- why this transaction receives this probability,\n",
        "- which factors increase or decrease suspicion,\n",
        "- using clear, neutral compliance-style language,\n",
        "- without mentioning SHAP values, machine-learning models, or internal mechanisms.\n",
        "\n",
        "Transaction ID: {tx_id}\n",
        "Model suspiciousness probability: {prob:.4f}\n",
        "\n",
        "Top contributing factors:\n",
        "{features_block}\n",
        "\n",
        "Respond in EXACTLY this format:\n",
        "\n",
        "Short explanation:\n",
        "<2–3 sentences, no numbering, no headings, no extra sections.>\n",
        "\n",
        "Key risk-relevant factors:\n",
        "- <bullet 1>\n",
        "- <bullet 2>\n",
        "- <bullet 3>\n",
        "(Use 3–6 bullets.)\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# LLM Explanation Generator\n",
        "# --------------------------------------------------------------------\n",
        "def generate_llm_explanations(local_explanations, model_name=\"gpt-5-mini\", save_json=True):\n",
        "    \"\"\"\n",
        "    Generate natural-language AML explanations using OpenAI's Responses API.\n",
        "    \"\"\"\n",
        "    llm_explanations = {}\n",
        "\n",
        "    for tx_id, expl_entry in local_explanations.items():\n",
        "        prompt = _build_llm_prompt_for_transaction(tx_id, expl_entry)\n",
        "\n",
        "        response = client.responses.create(\n",
        "            model=model_name,\n",
        "            input=prompt,\n",
        "        )\n",
        "\n",
        "        raw = response.output_text.strip()\n",
        "\n",
        "        # ---- Split according to the enforced format ----\n",
        "        short_header = \"Short explanation:\"\n",
        "        factors_header = \"Key risk-relevant factors:\"\n",
        "\n",
        "        if short_header in raw and factors_header in raw:\n",
        "            before, after = raw.split(factors_header, 1)\n",
        "            short_text = before.replace(short_header, \"\").strip()\n",
        "            detailed_text = \"Key risk-relevant factors:\\n\" + after.strip()\n",
        "        else:\n",
        "            # fallback (should rarely trigger)\n",
        "            parts = raw.split(\"\\n\\n\", 1)\n",
        "            short_text = parts[0].strip()\n",
        "            detailed_text = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "        llm_explanations[tx_id] = {\n",
        "            \"probability\": expl_entry[\"probability\"],\n",
        "            \"top_features\": expl_entry[\"top_features\"],\n",
        "            \"explanation_short\": short_text,\n",
        "            \"explanation_detailed\": detailed_text,\n",
        "        }\n",
        "\n",
        "        # Save JSON for auditability\n",
        "        if save_json:\n",
        "            out_path = os.path.join(LLM_LOCAL_DIR, f\"hi_small_llm_tx_{tx_id}.json\")\n",
        "            with open(out_path, \"w\") as f:\n",
        "                json.dump(llm_explanations[tx_id], f, indent=2)\n",
        "\n",
        "    print(\"[INFO] Generated LLM explanations for:\", list(local_explanations.keys()))\n",
        "    if save_json:\n",
        "        print(\"[INFO] Saved JSON files under:\", LLM_LOCAL_DIR)\n",
        "\n",
        "    return llm_explanations"
      ],
      "metadata": {
        "id": "cSvIpv1UCGTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Section 13: Test Call (LLM)\n",
        "# -------------------------\n",
        "\n",
        "def pretty_print_explanation(tx_id, expl_entry):\n",
        "    \"\"\"\n",
        "    Nicely format a single transaction explanation in the notebook output.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Transaction ID: {tx_id}\")\n",
        "    print(f\"Model suspiciousness probability: {expl_entry['probability']:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"Short explanation:\\n\")\n",
        "    print(expl_entry[\"explanation_short\"])\n",
        "    print(\"\\nDetailed explanation:\\n\")\n",
        "    print(expl_entry[\"explanation_detailed\"])\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Example transaction IDs (replace with any IDs you like)\n",
        "test_ids = [298872, 746726, 405190]\n",
        "\n",
        "# 1) Local SHAP → LLM pipeline\n",
        "local_expl = explain_transactions_local(test_ids, top_n=5)\n",
        "llm_expl = generate_llm_explanations(local_expl, model_name=\"gpt-5-mini\")\n",
        "\n",
        "# 2) Pretty print the explanations\n",
        "for tx_id in test_ids:\n",
        "    pretty_print_explanation(tx_id, llm_expl[tx_id])"
      ],
      "metadata": {
        "id": "gr5PG6ALCVN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section transforms the *local SHAP* outputs into short, compliance-style AML\n",
        "explanations using OpenAI’s **Responses API**.\n",
        "\n",
        "### `generate_llm_explanations(local_explanations, model_name=\"gpt-5-mini\", save_json=True)`\n",
        "\n",
        "**Inputs**\n",
        "- `local_explanations`: output of `explain_transactions_local(...)` containing\n",
        "  the model probability and top SHAP features per transaction.\n",
        "- `model_name`: GPT-5 model to use (`gpt-5-mini` recommended).\n",
        "- `save_json`: if `True`, stores one JSON explanation per transaction under\n",
        "  `llm_outputs/local/`.\n",
        "\n",
        "**Outputs**\n",
        "Each transaction receives:\n",
        "- the predicted suspiciousness probability,\n",
        "- the top contributing SHAP features,\n",
        "- a concise 2–3 sentence explanation,\n",
        "- a structured (bullet-style) explanation.\n",
        "\n",
        "This output can be directly used for UI prototypes, audit tables, or thesis\n",
        "screenshots."
      ],
      "metadata": {
        "id": "0JI8Lk7ACHso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 14: Unified SHAP → LLM Runner"
      ],
      "metadata": {
        "id": "1QZfhSwuAKBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_explanation_pipeline(transaction_ids, top_n=5, model_name=\"gpt-5-mini\"):\n",
        "\n",
        "    # --- Normalize input ---\n",
        "    if isinstance(transaction_ids, (int, str)):\n",
        "        tx_ids = [transaction_ids]\n",
        "    elif isinstance(transaction_ids, list):\n",
        "        tx_ids = transaction_ids\n",
        "    else:\n",
        "        raise ValueError(\"transaction_ids must be int, str, or list of these.\")\n",
        "\n",
        "    # --- Local SHAP ---\n",
        "    print(\"[INFO] Running local SHAP...\")\n",
        "    local_expl = explain_transactions_local(tx_ids, top_n=top_n)\n",
        "\n",
        "    # --- LLM explanations ---\n",
        "    print(\"[INFO] Calling LLM (\", model_name, \") ...\")\n",
        "    llm_expl = generate_llm_explanations(local_expl, model_name=model_name)\n",
        "\n",
        "    # --- Pretty output ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"\\n=== FINAL NATURAL-LANGUAGE EXPLANATIONS ===\\n\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for tx_id in tx_ids:\n",
        "        entry = llm_expl[tx_id]\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"Transaction ID: {tx_id}\")\n",
        "        print(f\"Suspiciousness probability: {entry['probability']:.4f}\\n\")\n",
        "\n",
        "        print(\"Short explanation:\")\n",
        "        print(entry[\"explanation_short\"], \"\\n\")\n",
        "\n",
        "        print(\"Detailed explanation:\")\n",
        "        print(entry[\"explanation_detailed\"])\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    return llm_expl"
      ],
      "metadata": {
        "id": "gFjcyUpbAJtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_explanation_pipeline([298872, 746726, 405190])"
      ],
      "metadata": {
        "id": "9aRYz7qaD38k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This unified runner cell provides a single entry point for producing a complete\n",
        "explanation for any transaction:\n",
        "\n",
        "1. It computes **local SHAP values** for the selected transaction(s).\n",
        "2. It calls the **LLM explanation engine** to generate natural-language\n",
        "   reasoning.\n",
        "3. It prints the final rationale in a clean, consistent format.\n",
        "4. It returns the explanation dictionary for further use (UI, export, etc.).\n",
        "\n",
        "The widgets (next section) will call this function directly."
      ],
      "metadata": {
        "id": "R7xc4u13AQl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 15: Scoring & Ranking – Model Probabilities"
      ],
      "metadata": {
        "id": "mI3hl9sEqykG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Sanity checks\n",
        "assert \"model\" in globals(), \"Model not found. Please run the training section first.\"\n",
        "assert \"X_full\" in globals(), \"X_full not found. Please run the data loading section first.\"\n",
        "assert \"transaction_ids\" in globals(), \"transaction_ids not found.\"\n",
        "assert \"y_full\" in globals(), \"y_full not found.\"\n",
        "assert \"df_full\" in globals(), \"df_full not found.\"\n",
        "\n",
        "print(\"[INFO] Scoring all transactions with XGBoost model...\")\n",
        "\n",
        "# 1) Predict suspiciousness probabilities for all transactions\n",
        "probs = model.predict_proba(X_full)[:, 1]  # probability of class '1' (suspicious)\n",
        "\n",
        "# 2) Base scoring DataFrame\n",
        "scores_df = pd.DataFrame({\n",
        "    \"transaction_id\": transaction_ids,\n",
        "    \"probability\": probs,\n",
        "    \"label\": y_full.values,\n",
        "})\n",
        "\n",
        "# 3) Context columns: amounts + simple flags\n",
        "context_cols = []\n",
        "for col in [\"Amount Paid\", \"Amount Received\", \"SameAccount\", \"SameBank\", \"SameCurrency\"]:\n",
        "    if col in df_full.columns:\n",
        "        context_cols.append(col)\n",
        "\n",
        "# 4) Payment currency one-hot columns (for UI display)\n",
        "currency_cols = [c for c in df_full.columns if c.startswith(\"Payment Currency_\")]\n",
        "\n",
        "merge_cols = [\"transaction_id\"] + context_cols + currency_cols\n",
        "\n",
        "# Merge context + currency info into scores_df\n",
        "scores_df = scores_df.merge(\n",
        "    df_full[merge_cols],\n",
        "    on=\"transaction_id\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "# 5) Sort by suspiciousness probability (descending)\n",
        "scores_df = scores_df.sort_values(\"probability\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 6) Save to disk for UI\n",
        "scores_path = os.path.join(\"data\", \"processed\", \"transaction_scores.csv\")\n",
        "os.makedirs(os.path.dirname(scores_path), exist_ok=True)\n",
        "scores_df.to_csv(scores_path, index=False)\n",
        "\n",
        "print(\"[INFO] Saved ranked transaction scores to:\", scores_path)\n",
        "print(\"[INFO] DataFrame shape:\", scores_df.shape)\n",
        "\n",
        "print(\"\\n[INFO] Top 20 transactions by model suspiciousness probability:\")\n",
        "scores_df.head(20)"
      ],
      "metadata": {
        "id": "tU8p2M_xqyQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section scores **all transactions** with the trained XGBoost model and\n",
        "creates a ranked list by suspiciousness probability.\n",
        "\n",
        "- It computes `predict_proba(X_full)[:, 1]` for every transaction.\n",
        "- It builds a compact table with:\n",
        "  - `transaction_id`\n",
        "  - model probability (`probability`)\n",
        "  - ground-truth label (`label`)\n",
        "  - a few context features (e.g. `Amount Paid`, `SameAccount`, `SameBank`, if present)\n",
        "- It sorts the table in **descending** order of probability and saves it as  \n",
        "  `data/processed/transaction_scores.csv`.\n",
        "\n",
        "The interactive UI will use this ranked table to let an analyst select one or\n",
        "more **high-risk transactions** for detailed SHAP + LLM explanations."
      ],
      "metadata": {
        "id": "4swPFzUzq20A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 16: Ranked Alerts UI – Top-k + Explain"
      ],
      "metadata": {
        "id": "sozkbScTEbgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- Load ranked scores ---\n",
        "scores_path = os.path.join(\"data\", \"processed\", \"transaction_scores.csv\")\n",
        "scores_df_ui = pd.read_csv(scores_path)\n",
        "\n",
        "# Ensure sorted by probability descending\n",
        "scores_df_ui = scores_df_ui.sort_values(\"probability\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Precompute valid transaction_ids for sanity checks\n",
        "valid_tx_ids = set(scores_df_ui[\"transaction_id\"].astype(int))\n",
        "\n",
        "# Helper: find payment currency name for a row\n",
        "currency_cols_ui = [c for c in scores_df_ui.columns if c.startswith(\"Payment Currency_\")]\n",
        "\n",
        "def get_payment_currency(row):\n",
        "    for col in currency_cols_ui:\n",
        "        # some rows might be float NaN → treat as 0\n",
        "        val = row.get(col, 0)\n",
        "        try:\n",
        "            is_one = float(val) == 1.0\n",
        "        except Exception:\n",
        "            is_one = False\n",
        "        if is_one:\n",
        "            # \"Payment Currency_US Dollar\" -> \"US Dollar\"\n",
        "            return col.replace(\"Payment Currency_\", \"\")\n",
        "    return \"Unknown\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# UI ELEMENTS\n",
        "# ---------------------------\n",
        "\n",
        "# How many alerts to show\n",
        "topk_slider = widgets.IntSlider(\n",
        "    value=20,\n",
        "    min=5,\n",
        "    max=100,\n",
        "    step=5,\n",
        "    description=\"Top-k alerts:\",\n",
        "    continuous_update=False,\n",
        "    style={\"description_width\": \"120px\"},\n",
        ")\n",
        "\n",
        "# Multi-select list of alerts\n",
        "alerts_select = widgets.SelectMultiple(\n",
        "    options=[],\n",
        "    description=\"Select transaction(s):\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"260px\"),\n",
        "    style={\"description_width\": \"140px\"},\n",
        ")\n",
        "\n",
        "# Manual ID input (comma-separated)\n",
        "manual_ids_text = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"e.g. 4102741, 298872, 746726\",\n",
        "    description=\"Manual IDs:\",\n",
        "    style={\"description_width\": \"140px\"},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "\n",
        "manual_hint_html = widgets.HTML(\n",
        "    \"<p style='font-size:11px;color:gray;'>\"\n",
        "    \"Enter transaction IDs as a comma-separated list. \"\n",
        "    \"Example: <code>4102741, 298872, 746726</code>.\"\n",
        "    \"</p>\"\n",
        ")\n",
        "\n",
        "# Run button\n",
        "explain_button = widgets.Button(\n",
        "    description=\"Explain selected\",\n",
        "    button_style=\"primary\",\n",
        "    tooltip=\"Run SHAP + LLM explanation pipeline\",\n",
        "    icon=\"play\",\n",
        ")\n",
        "\n",
        "# Output area\n",
        "alerts_output = widgets.Output()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Helper: update alert list\n",
        "# ---------------------------\n",
        "def update_alert_list(*args):\n",
        "    k = topk_slider.value\n",
        "    top = scores_df_ui.head(k)\n",
        "\n",
        "    options = []\n",
        "    for rank, (_, row) in enumerate(top.iterrows(), start=1):\n",
        "        tx_id = int(row[\"transaction_id\"])\n",
        "        prob = row[\"probability\"]\n",
        "        label_val = int(row[\"label\"])\n",
        "\n",
        "        label_str = \"1 (laundering)\" if label_val == 1 else \"0 (normal)\"\n",
        "\n",
        "        # Amount Paid + currency (if available)\n",
        "        if \"Amount Paid\" in row and not pd.isna(row[\"Amount Paid\"]):\n",
        "            amount_val = row[\"Amount Paid\"]\n",
        "            currency_name = get_payment_currency(row)\n",
        "            amount_part = f\"Amount Paid: {amount_val:,.2f} {currency_name}\"\n",
        "        else:\n",
        "            amount_part = \"Amount Paid: n/a\"\n",
        "\n",
        "        option_label = (\n",
        "            f\"Rank #{rank} | Transaction ID: {tx_id} | \"\n",
        "            f\"Probability: {prob:.4f} | Label: {label_str} | {amount_part}\"\n",
        "        )\n",
        "        option_value = tx_id\n",
        "\n",
        "        options.append((option_label, option_value))\n",
        "\n",
        "    alerts_select.options = options\n",
        "\n",
        "# Initialize & bind\n",
        "update_alert_list()\n",
        "topk_slider.observe(update_alert_list, names=\"value\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Button callback\n",
        "# ---------------------------\n",
        "def on_explain_clicked(b):\n",
        "    with alerts_output:\n",
        "        clear_output()\n",
        "\n",
        "        # IDs from the list\n",
        "        selected_list = list(alerts_select.value)\n",
        "\n",
        "        # IDs from manual text\n",
        "        manual_raw = manual_ids_text.value.strip()\n",
        "        manual_ids = []\n",
        "        if manual_raw:\n",
        "            parts = manual_raw.split(\",\")\n",
        "            for p in parts:\n",
        "                p = p.strip()\n",
        "                if not p:\n",
        "                    continue\n",
        "                try:\n",
        "                    manual_ids.append(int(p))\n",
        "                except ValueError:\n",
        "                    print(f\"[WARN] Could not parse '{p}' as an integer transaction_id; ignored.\")\n",
        "\n",
        "        # Combine + deduplicate\n",
        "        combined_ids = list(sorted(set(selected_list + manual_ids)))\n",
        "\n",
        "        if not combined_ids:\n",
        "            print(\"[WARN] No transactions selected. Use the list or enter IDs manually.\")\n",
        "            return\n",
        "\n",
        "        # Filter invalid IDs (not in dataset)\n",
        "        invalid_ids = [tx for tx in combined_ids if tx not in valid_tx_ids]\n",
        "        if invalid_ids:\n",
        "            print(\"[WARN] The following IDs are not present in the dataset and will be ignored:\")\n",
        "            print(\" \", invalid_ids)\n",
        "            combined_ids = [tx for tx in combined_ids if tx in valid_tx_ids]\n",
        "\n",
        "        if not combined_ids:\n",
        "            print(\"[ERROR] After filtering invalid IDs, nothing remains to explain.\")\n",
        "            return\n",
        "\n",
        "        print(\"[INFO] Using transaction_ids:\", combined_ids)\n",
        "        print(\n",
        "            \"[INFO] Running full SHAP → LLM pipeline with gpt-5-mini \"\n",
        "            \"(top-5 SHAP features)...\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            _ = run_explanation_pipeline(\n",
        "                combined_ids,\n",
        "                top_n=5,              # fixed top-5 SHAP features\n",
        "                model_name=\"gpt-5-mini\",\n",
        "            )\n",
        "            print(\"\\n[INFO] Done.\")\n",
        "        except Exception as e:\n",
        "            print(\"\\n[ERROR] Pipeline failed:\", e)\n",
        "\n",
        "\n",
        "explain_button.on_click(on_explain_clicked)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Layout\n",
        "# ---------------------------\n",
        "header_html = widgets.HTML(\n",
        "    \"<h3>Ranked AML Alerts – Select & Explain</h3>\"\n",
        "    \"<p style='font-size:12px;'>\"\n",
        "    \"Transactions are sorted by the model's predicted suspiciousness probability \"\n",
        "    \"(highest first). True labels are shown only because this is a labelled \"\n",
        "    \"benchmark dataset; in production, labels would not be known at alert time.\"\n",
        "    \"</p>\"\n",
        "    \"<p style='font-size:11px;color:gray;'>\"\n",
        "    \"Tip: hold Ctrl (or ⌘ on Mac) to select multiple transactions in the list.\"\n",
        "    \"</p>\"\n",
        ")\n",
        "\n",
        "footer_html = widgets.HTML(\n",
        "    \"<p style='font-size:11px;color:gray;'>\"\n",
        "    \"Each explanation uses the top 5 SHAP features for that transaction. \"\n",
        "    \"Model: XGBoost baseline on HI-Small (ROC-AUC ≈ 0.93). \"\n",
        "    \"Explanation model: GPT-5-mini.\"\n",
        "    \"</p>\"\n",
        ")\n",
        "\n",
        "alerts_ui = widgets.VBox([\n",
        "    header_html,\n",
        "    topk_slider,\n",
        "    alerts_select,\n",
        "    manual_ids_text,\n",
        "    manual_hint_html,\n",
        "    explain_button,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    alerts_output,\n",
        "    footer_html,\n",
        "])\n",
        "\n",
        "display(alerts_ui)"
      ],
      "metadata": {
        "id": "yo8ubvREEbxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This UI provides two ways to select transactions for explanation:\n",
        "\n",
        "1. **Ranked alert list**  \n",
        "   - The top-k most suspicious transactions (based on model probability) are shown.  \n",
        "   - You can select one or multiple transactions directly from the list  \n",
        "     (hold **Ctrl** / **⌘** for multi-select).\n",
        "\n",
        "2. **Manual ID input**  \n",
        "   - Enter any transaction IDs as a comma-separated list.  \n",
        "   - Useful for testing random cases, edge cases, or custom evaluation scenarios.\n",
        "\n",
        "After selecting IDs (from the list, manual input, or both),  \n",
        "click **“Explain selected”** to run the full SHAP → GPT-5 explanation pipeline.\n",
        "\n",
        "True labels are shown only because this is a benchmark dataset.  \n",
        "In real-world AML systems, labels are not available at alert time."
      ],
      "metadata": {
        "id": "XyfI1lLjr0Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper code: Commit and push changes back to GitHub"
      ],
      "metadata": {
        "id": "gXlPIJoeklxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Load token\n",
        "GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
        "assert GITHUB_TOKEN, \"Please set GITHUB_TOKEN in Colab secrets.\"\n",
        "\n",
        "# Build the authenticated remote URL\n",
        "remote_url = f\"https://{GITHUB_TOKEN}:x-oauth-basic@github.com/ZeusGav/BA_test.git\"\n",
        "\n",
        "# Update remote URL\n",
        "!git remote set-url origin \"{remote_url}\"\n",
        "\n",
        "# Stage changes\n",
        "!git add .\n",
        "\n",
        "# Commit\n",
        "!git commit -m \"Auto-commit: updated project files\" || echo \"Nothing to commit.\"\n",
        "\n",
        "# Push\n",
        "!git push origin main"
      ],
      "metadata": {
        "id": "Vd7998PTesED",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This utility cell commits and pushes all tracked project files to GitHub.  \n",
        "Large artifacts (datasets, SHAP arrays, model binaries, LLM outputs) are excluded through `.gitignore`, ensuring the repository stays lightweight and reproducible."
      ],
      "metadata": {
        "id": "MgseptltkqW-"
      }
    }
  ]
}